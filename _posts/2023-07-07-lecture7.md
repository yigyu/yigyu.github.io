---
layout: single
title: "stanford cs231n lecture7"
categories: cs231n
tags: [computer vision]
---

# Lecture7. Training Neural Networks part 2

## Fancier optimization

### SGD(Stochastic Gradient Descent)

![SGD](/assets/images/2023-07-07-lecture7/SGD.jpg)

### SGD + Momentum & NAG(Nesterov Accelerated Gradient)

![SGD + Momentum](/assets/images/2023-07-07-lecture7/Momentum.jpg)

### AdaGrad

![AdaGrad](/assets/images/2023-07-07-lecture7/AdaGrad.jpg)

### RMSProp

![RMSProp](/assets/images/2023-07-07-lecture7/RMSProp.jpg)

### Adam

![Adam](/assets/images/2023-07-07-lecture7/Adam.jpg)
![Adam2](/assets/images/2023-07-07-lecture7/Adam2.jpg)

### Learning rate
SGD, SGD+Momentum, Adagrad, RMSProp, Adam all have learning rate as a hyperparameter

![Learningrate](/assets/images/2023-07-07-lecture7/Learning rate.jpg)

## Regularization

### Dropout

![Regularization](/assets/images/2023-07-07-lecture7/Regularization.jpg)

## Transfer learning

![Transferlearning](/assets/images/2023-07-07-lecture7/Transfer learning.jpg)

lecture : [https://youtu.be/_JB0AO7QxSA](https://youtu.be/_JB0AO7QxSA)

note : [http://cs231n.stanford.edu/slides/2023/lecture_7.pdf](http://cs231n.stanford.edu/slides/2023/lecture_7.pdf)